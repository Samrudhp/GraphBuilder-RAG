"""
Test Actual LLM Output Generation

Shows real LLM responses when given hybrid context (graph + semantic chunks).
Tests what the LLM actually outputs when answering questions.
"""

import asyncio
import sys
import os

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

print("ğŸ¤– Testing Actual LLM Output Generation")
print("=" * 80)


async def test_real_llm_responses():
    """Test real LLM outputs with hybrid context."""
    
    try:
        from services.ingestion.service import IngestionService
        from services.normalization.service import NormalizationService
        from services.extraction.service import ExtractionService
        from services.query.service import QueryService
        from shared.models.schemas import DocumentType, QueryRequest
        from shared.database.neo4j import get_neo4j
        import tempfile
        from pathlib import Path
        
        print("\nğŸ“‹ Step 1: Setup Test Document & Knowledge Graph")
        print("-" * 80)
        
        # Create test document
        test_content = """
        Albert Einstein was born in Ulm, Germany on March 14, 1879. He developed 
        the theory of relativity, which revolutionized modern physics. Einstein 
        received the Nobel Prize in Physics in 1921 for his explanation of the 
        photoelectric effect.
        
        In 1905, known as Einstein's "miracle year," he published four groundbreaking 
        papers covering the photoelectric effect, Brownian motion, special relativity, 
        and the famous equation E=mcÂ².
        
        Einstein worked at the Swiss Patent Office in Bern from 1902 to 1909 while 
        pursuing his doctorate. He later held professorships at universities in 
        Zurich, Prague, and Berlin.
        
        Due to the rise of Nazi Germany, Einstein emigrated to the United States 
        in 1933. He settled at Princeton University where he continued his research 
        until his death in 1955. Einstein was not only a brilliant physicist but 
        also a passionate advocate for peace and civil rights.
        """
        
        # Clean up previous test data
        print("   Cleaning previous test data...")
        neo4j_connector = get_neo4j()
        
        with neo4j_connector.get_session() as session:
            session.run("""
                MATCH (n)
                WHERE n.test_id = 'llm_output_test'
                DETACH DELETE n
            """)
        
        # Initialize services
        ingestion = IngestionService()
        normalization = NormalizationService()
        extraction = ExtractionService()
        query_service = QueryService()
        
        # Ingest and process document
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(test_content)
            temp_file = f.name
        
        try:
            print("   Ingesting document...")
            result = await ingestion.ingest_from_file(Path(temp_file), DocumentType.TEXT)
            raw_doc_id = result.document_id
            
            print("   Normalizing document...")
            norm_result = await normalization.normalize_document(raw_doc_id)
            normalized_doc_id = norm_result.document_id
            
            print("   Extracting triples with LLM...")
            candidate_triples = await extraction.extract_from_document(normalized_doc_id)
            print(f"   âœ… Extracted {len(candidate_triples)} triples")
            
            # Insert into knowledge graph
            print("   Building knowledge graph...")
            entities = {}
            for candidate in candidate_triples:
                triple = candidate.triple
                if triple.subject not in entities:
                    entities[triple.subject] = triple.subject_type if triple.subject_type else 'Other'
                if triple.object not in entities:
                    entities[triple.object] = triple.object_type if triple.object_type else 'Other'
            
            # Create entities
            with neo4j_connector.get_session() as session:
                for entity_name, entity_type in entities.items():
                    entity_id = f"entity_{hash(entity_name)}"
                    type_str = entity_type.value if hasattr(entity_type, 'value') else str(entity_type)
                    
                    session.run(f"""
                        MERGE (e:{type_str} {{
                            entity_id: $entity_id,
                            canonical_name: $name,
                            entity_type: $type,
                            test_id: 'llm_output_test'
                        }})
                    """, entity_id=entity_id, name=entity_name, type=type_str)
            
            # Create relationships
            with neo4j_connector.get_session() as session:
                for candidate in candidate_triples:
                    triple = candidate.triple
                    
                    subject_id = f"entity_{hash(triple.subject)}"
                    object_id = f"entity_{hash(triple.object)}"
                    
                    # Clean relationship type - remove invalid characters
                    rel_type = triple.predicate.upper()
                    rel_type = rel_type.replace(" ", "_").replace("-", "_").replace(".", "")
                    rel_type = rel_type.replace(",", "_").replace("(", "").replace(")", "")
                    rel_type = rel_type.replace("/", "_").replace("'", "").replace('"', "")
                    rel_type = ''.join(c for c in rel_type if c.isalnum() or c == '_')[:50]
                    if not rel_type:
                        rel_type = "RELATED_TO"
                    
                    evidence_str = ""
                    if candidate.evidence:
                        if isinstance(candidate.evidence, list):
                            evidence_str = "; ".join([
                                ev.text if hasattr(ev, 'text') else str(ev)
                                for ev in candidate.evidence
                            ])
                        else:
                            evidence_str = str(candidate.evidence)
                    
                    session.run(f"""
                        MATCH (subject {{entity_id: $subject_id, test_id: 'llm_output_test'}})
                        MATCH (object {{entity_id: $object_id, test_id: 'llm_output_test'}})
                        MERGE (subject)-[r:{rel_type} {{
                            predicate: $predicate,
                            confidence: $confidence,
                            evidence: $evidence,
                            test_id: 'llm_output_test'
                        }}]->(object)
                    """,
                    subject_id=subject_id,
                    object_id=object_id,
                    predicate=triple.predicate,
                    confidence=candidate.confidence,
                    evidence=evidence_str[:500])
            
            print(f"   âœ… Knowledge graph built: {len(entities)} entities, {len(candidate_triples)} relationships")
            
            # Now test actual LLM Q&A
            print("\n" + "=" * 80)
            print("ğŸ¤– TESTING REAL LLM OUTPUTS")
            print("=" * 80)
            
            # Test Question 1: Simple factual question
            print("\n" + "â”€" * 80)
            print("ğŸ“ Question 1: Where was Albert Einstein born?")
            print("â”€" * 80)
            
            query_request = QueryRequest(
                question="Where was Albert Einstein born?",
                max_chunks=5,
                require_verification=True
            )
            
            print("\nğŸ” Retrieving context from graph and semantic search...")
            result1 = await query_service.answer_question(query_request)
            
            print("\nğŸ“Š Retrieved Context:")
            # Note: context is embedded in the service, not exposed in response
            print(f"   â€¢ Response received with answer")
            
            print("\nğŸ¤– LLM ANSWER:")
            print("â”Œ" + "â”€" * 78 + "â”")
            for line in result1.answer.split('\n'):
                print(f"â”‚ {line[:76]:<76} â”‚")
            print("â””" + "â”€" * 78 + "â”˜")
            
            print(f"\nâœ“ Verification: {result1.verification_status.value}")
            print(f"  Score: {result1.confidence:.2f}")
            print(f"  Processing time: {result1.processing_time_ms}ms")
            
            # Test Question 2: Multi-fact question
            print("\n" + "â”€" * 80)
            print("ğŸ“ Question 2: What were Einstein's major achievements?")
            print("â”€" * 80)
            
            query_request2 = QueryRequest(
                question="What were Einstein's major achievements in physics?",
                max_chunks=5,
                require_verification=True
            )
            
            print("\nğŸ” Retrieving context...")
            result2 = await query_service.answer_question(query_request2)
            
            print(f"\nğŸ“Š Query processed in {result2.processing_time_ms}ms")
            
            print("\nğŸ¤– LLM ANSWER:")
            print("â”Œ" + "â”€" * 78 + "â”")
            for line in result2.answer.split('\n'):
                print(f"â”‚ {line[:76]:<76} â”‚")
            print("â””" + "â”€" * 78 + "â”˜")
            
            print(f"\nâœ“ Verification: {result2.verification_status.value}")
            print(f"  Score: {result2.confidence:.2f}")
            
            # Test Question 3: Timeline question
            print("\n" + "â”€" * 80)
            print("ğŸ“ Question 3: Tell me about Einstein's career timeline")
            print("â”€" * 80)
            
            query_request3 = QueryRequest(
                question="Can you describe Einstein's career timeline and where he worked?",
                max_chunks=5,
                require_verification=True
            )
            
            print("\nğŸ” Retrieving context...")
            result3 = await query_service.answer_question(query_request3)
            
            print(f"\nğŸ“Š Query processed in {result3.processing_time_ms}ms")
            
            print("\nğŸ¤– LLM ANSWER:")
            print("â”Œ" + "â”€" * 78 + "â”")
            for line in result3.answer.split('\n'):
                print(f"â”‚ {line[:76]:<76} â”‚")
            print("â””" + "â”€" * 78 + "â”˜")
            
            print(f"\nâœ“ Verification: {result3.verification_status.value}")
            print(f"  Score: {result3.confidence:.2f}")
            
            # Test Question 4: Question that might cause hallucination
            print("\n" + "â”€" * 80)
            print("ğŸ“ Question 4: What was Einstein's relationship with Nikola Tesla?")
            print("â”€" * 80)
            print("   (Testing hallucination detection - this info is NOT in our document)")
            
            query_request4 = QueryRequest(
                question="What was Einstein's relationship with Nikola Tesla?",
                max_chunks=5,
                require_verification=True
            )
            
            print("\nğŸ” Retrieving context...")
            result4 = await query_service.answer_question(query_request4)
            
            print(f"\nğŸ“Š Query processed in {result4.processing_time_ms}ms")
            
            print("\nğŸ¤– LLM ANSWER:")
            print("â”Œ" + "â”€" * 78 + "â”")
            for line in result4.answer.split('\n'):
                print(f"â”‚ {line[:76]:<76} â”‚")
            print("â””" + "â”€" * 78 + "â”˜")
            
            print(f"\nâœ“ Verification: {result4.verification_status.value}")
            print(f"  Score: {result4.confidence:.2f}")
            if result4.verification_status.value == "UNSUPPORTED":
                print("  âš ï¸  LLM response contains unsupported claims (potential hallucination)")
            
            # Summary
            print("\n" + "=" * 80)
            print("âœ… LLM OUTPUT TEST COMPLETE")
            print("=" * 80)
            print("""
ğŸ“Š What we tested:
   1. Simple factual question â†’ LLM provides direct answer
   2. Multi-fact question â†’ LLM combines multiple facts
   3. Timeline question â†’ LLM synthesizes chronological narrative
   4. Out-of-scope question â†’ LLM should indicate insufficient info
   
ğŸ’¡ Key observations:
   â€¢ LLM receives BOTH graph facts and semantic chunks
   â€¢ Answers are grounded in retrieved context
   â€¢ Verification checks claims against knowledge graph
   â€¢ Hallucinations detected when LLM goes beyond available data
   
ğŸ¯ The hybrid approach works:
   â€¢ Graph provides: Structured facts with confidence
   â€¢ Semantic provides: Rich context and details
   â€¢ LLM combines: Comprehensive, natural answers
   â€¢ Verification ensures: Answers stay grounded in truth
            """)
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
        
        # Cleanup
        print("\nğŸ§¹ Cleaning up test data...")
        with neo4j_connector.get_session() as session:
            result = session.run("""
                MATCH (n)
                WHERE n.test_id = 'llm_output_test'
                DETACH DELETE n
                RETURN count(n) as deleted
            """)
            record = result.single()
            print(f"âœ… Cleaned up {record['deleted']} test nodes")
        
        return True
    
    except Exception as e:
        print(f"\nâŒ Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run test."""
    try:
        success = await test_real_llm_responses()
        
        if success:
            print("\nâœ… All LLM output tests passed!")
            return 0
        else:
            print("\nâŒ Some tests failed")
            return 1
    
    except Exception as e:
        print(f"\nâŒ Test suite failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
